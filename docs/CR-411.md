Awesome, let’s make this thing *social* and give you a proper “director’s console” for the brain.

Here’s a new batch of change requests starting at **CR‑411**, focused on:

- Multi‑user / multi‑device collaboration.
- A **Director Mode** where a web client or another Vision Pro can “drive” the scene.
- Recording / replay of guided tours.

All of this stays grounded in the H3LIX / LAIZA / MPG / RV / MUFS architecture.  [oai_citation:0‡Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  

---

## CR‑411 – Collaboration & Roles: Director vs Participant Sessions

**Status:** Proposed  
**Type:** Architecture / Collaboration  
**Baseline:**  
- Single‑user Vision Pro app (WindowGroup + ImmersiveSpace).  
- Web viewer using the same telemetry APIs.  
- H3LIX stack (telemetry, MPG, RV, MUFS, SORK‑N).  [oai_citation:1‡Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  

### 1. Goal

Add a **collaboration layer** that supports:

- One or more **Directors** (web app or Vision Pro) controlling:
  - Time (live vs replay).
  - Focus (which RV/MUFS/segment/decision is highlighted).
  - Visual filters & modes.
- Multiple **Participants** (Vision Pro and web clients) passively following those controls, while still being able to look around locally.

### 2. New concepts

#### 2.1 Collaboration session

Introduce a new entity in the backend:

```ts
interface CollabSession {
  collab_id: string;
  experiment_id: string;
  session_id: string;       // the underlying H3LIX/LAIZA run
  created_by: string;       // user id of creator
  mode: "live" | "replay";
  active_director_id: string;
  participants: string[];   // client ids
  created_utc: string;
  updated_utc: string;
}
```

- A **collaboration session** is a layer on top of a normal H3LIX `session_id`.
- Multiple collab sessions can overlay the same underlying H3LIX session (e.g., different directors giving different tours).

#### 2.2 Roles

```ts
type CollabRole = "director" | "participant";
```

- A **Director**:
  - Sets global time `t_rel_ms`.
  - Chooses active mode (`live`, `replay`, `rogueInspect`, `mufsInspect`, etc.).
  - Picks active focus objects: `rogue_id`, `decision_id`, `mpg_center_node`, etc.
- A **Participant**:
  - Has **local camera freedom** (in Vision Pro: walk around / rotate head; in web: orbit camera).
  - Cannot override global state; can only request “local HUD” actions (like inspecting a node) which don’t affect others.

### 3. Shared “scene control” state

Create a small control document (per collab session):

```ts
interface SceneControlState {
  collab_id: string;
  session_id: string;

  mode: "live" | "replay" | "rogueInspect" | "mufsInspect";
  t_rel_ms: number;

  // global focus
  focus?: {
    rogue_id?: string;
    decision_id?: string;
    mpg_level?: number;
    center_node_id?: string;
  };

  // visualization flags
  show_mufs_ghost_view: boolean;
  show_noetic_halo: boolean;
  show_body_ghost: boolean;
  show_sork_ring: boolean;

  // playback
  playback_speed: number;   // e.g., 0.25, 1.0, 2.0
  is_playing: boolean;
}
```

- This is a **small, infrequently changing** state machine we can broadcast to all clients.
- Directors update it; participants subscribe.

### 4. Backend support

New endpoints:

```http
POST   /v1/collab              # create a new CollabSession
GET    /v1/collab/{collab_id}  # details
POST   /v1/collab/{collab_id}/join     # join as director or participant
POST   /v1/collab/{collab_id}/control  # director updates SceneControlState
GET    /v1/collab/{collab_id}/control  # latest SceneControlState snapshot
```

Plus a **WebSocket channel** for control state:

```http
GET /v1/collab/{collab_id}/control/stream
```

Messages:

```json
{ "type": "control_state", "state": { /* SceneControlState */ } }
```

Directors send deltas, server rebroadcasts the full updated state.

### 5. Client behavior

- Web & Vision Pro clients:
  - On join, fetch `/control` (snapshot).
  - Subscribe to `/control/stream` for live updates.
  - Fuse `SceneControlState` with their local visual state (Helix/Halo/MGP/SORK) by setting:
    - Playback mode, `t_rel_ms`, focus objects, mode toggles.

### 6. Acceptance criteria

- Multiple participants see **the same focus & time** (e.g., same decision / Rogue Variable highlighted), but can physically stand in different spots in the spatial scene.
- Directors can hand off control (change `active_director_id`) without glitching clients.
- Control channel is robust: late‑joining participants quickly catch up via snapshot + stream.

---

## CR‑412 – Co‑located Multiuser Brain Viewing (Shared Spatial Anchor)

**Status:** Proposed  
**Type:** Spatial Collaboration / AR  
**Baseline:**  
- Single‑user Vision Pro ImmersiveSpace with `AnchorEntity(world: ...)`.  
- Collaboration model & SceneControl from CR‑411.

### 1. Goal

Let **multiple Vision Pro users in the same room** see **the same brain** in the same physical place:

- One shared helix + halo + MPG city, aligned to the same spatial anchor.
- Each user can walk around it; everyone points at the same structures.

### 2. Shared world alignment

For co‑located collaboration, use:

- A **shared reference point** in the room (e.g., a printed marker, a table).
- ARKit / RealityKit’s world‑tracking to create an `AnchorEntity` for that point on each headset.
- An **anchor sync mechanism**:

  - One device is “Host”:
    - Defines a world anchor (e.g., at a table center).
    - Generates a **short join code** or QR.
  - Other devices join:
    - Scan host’s QR or enter code.
    - Receive a **shared transform** (position + rotation) that defines where to place the root `AnchorEntity`.

We don’t need perfect world‑map merging; we just need everyone to use the **same local coordinate frame** relative to an agreed reference.

### 3. Scene placement

When user joins `collab_id` as a participant in “co‑located mode”:

- The app prompts them to either:
  - Align to a host (scan / accept host anchor), or
  - Become a host (create anchor).
- Once aligned:
  - The RealityKit `AnchorEntity` is positioned so the brain scene appears in the same spot.

### 4. Interaction affordances

- Use **in‑scene pointers**:
  - Each user’s gaze or pointing ray can be visualized as a thin beam with a small cursor, optionally colored per user.
  - When a user selects a node (pinch), a colored halo appears briefly, visible to everyone.
- For Director/Participant roles:
  - Director’s pointer is slightly thicker / more intense.
  - When Director selects something, it also updates global `SceneControlState.focus`.

### 5. Acceptance criteria

- Two Vision Pro devices in the same room see the brain structure anchored to the same physical location within a small error (~a few cm).
- When one participant points to a node/segment, others can see where they’re pointing.
- Director’s selections are globally reflected, while participants’ “pointing” is just ephemeral.

---

## CR‑413 – Web “Director Console” Driving Vision Pro Scenes

**Status:** Proposed  
**Type:** Cross‑Platform UX / Control  
**Baseline:**  
- Web 3D viewer (React + R3F) already displays the same data.  
- SceneControlState + CollabSession (CR‑411).

### 1. Goal

Turn the existing web viewer into a **Director Console**:

- Experimenter / analyst sits at a laptop.
- Vision Pro users wear headsets and “live” inside the brain.
- The web console drives:
  - Time (scrub, play).
  - Selected RV / MUFS / segment / decision.
  - Visualization toggles (halo on/off, MUFS ghost view, etc).

### 2. Web UI additions

In the React app:

- Add a **“Collaboration” panel**:
  - List existing `CollabSession`s (from `/v1/collab`).
  - Create new collab session (choose `session_id`, role=director).
  - See connected participants (client IDs, platform: web/vision).

- Add **Director controls**:
  - Timeline (already for replay) now **writes** to `SceneControlState`.
  - Buttons:
    - “Go Live”, “Pause Replay”.
    - “Focus on Rogue X” (from a list of RV events).
    - “Focus on Decision D” (list of decisions).
    - “Show MUFS Ghost View”, “Highlight RVs”, etc.

### 3. Data flow

- Web director client:
  - Maintains a local copy of `SceneControlState`.
  - On user interactions, sends patches to `/v1/collab/{id}/control`.
- Backend:
  - Validates commands (only active_director_id may send).
  - Updates canonical `SceneControlState`.
  - Broadcasts new state via `/control/stream`.
- Vision Pro participants:
  - Subscribe and apply state to their local `H3LIXPlaybackController` and visual state.

### 4. Optional “follow director camera” mode for web participants

- Additional flag: `follow_director_camera: boolean`.
- Web participants who opt‑in:
  - Have their orbit camera synced to director’s camera transforms (for demos).
- Vision Pro users are **never** hard‑locked to a remote camera; they always control their own physical viewpoint.

### 5. Acceptance criteria

- A director using the web app can:
  - Start a collab session.
  - See connected Vision Pro participants.
  - Scrub time + choose RV/MUFS/segments.
  - Observe participants’ scenes respond (highlighting, time plane, halo, etc).
- Participants’ head motion and local viewpoint remain under their own control.

---

## CR‑414 – “Tour Recording” for Guided Brain Walkthroughs

**Status:** Proposed  
**Type:** Recording / Playback  
**Baseline:**  
- SceneControlState + CollabSession.  
- Time navigation & playback (CR‑407).

### 1. Goal

Allow Directors to **record a “tour”** of the brain:

- A sequence of:
  - Time changes.
  - Focus changes (RVs, MUFS, decisions, segments).
  - Visualization toggles.
- Then **replay** this tour later for participants (live or asynchronous).

### 2. Tour model

```ts
interface SceneControlEvent {
  t_wall_ms: number;         // real time of command
  state_delta: Partial<SceneControlState>;
}

interface BrainTour {
  tour_id: string;
  collab_id: string;
  name: string;
  description?: string;
  created_by: string;
  created_utc: string;
  events: SceneControlEvent[];
}
```

### 3. Recording mechanism

- Director presses “Record Tour” in web console or Vision Pro HUD.
- From that moment:
  - Every change to `SceneControlState` (initiated by director) is logged as a `SceneControlEvent`.
- When finished:
  - Director presses “Stop & Save”.
  - Tour is persisted and associated with `collab_id` (and underlying `session_id`).

### 4. Playback

- Any Director can later:
  - Select a `BrainTour` and press “Play”.
  - System switches collab into **tour playback** mode:
    - Replay `SceneControlEvent`s at their original tempo (or scaled).
    - Broadcast resulting `SceneControlState` updates to all participants.
- Participants:
  - Experience a guided, smooth sequence of:
    - Time plane sweeps.
    - Focus jumps to RV/MUFS/decisions.
    - Halo/city view changes.
  - Still free to walk/look around; only global context is scripted.

### 5. Acceptance criteria

- Tours can be recorded over a single session; data stored in backend.
- Tour playback yields deterministic **global** behavior across all participants (global state matches recording).
- Directors can pause, resume, or scrub tours mid‑play.

---

## CR‑415 – Access Control, Privacy & Safety in Collaborative Mode

**Status:** Proposed  
**Type:** Security / Governance  
**Baseline:**  
- H3LIX sessions may contain **sensitive multimodal data** (physiology, text, MPG traits).  [oai_citation:2‡Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  
- Collaboration & Director Mode (CR‑411–414) increase the risk of oversharing.

### 1. Goal

Ensure collaborative & director features respect:

- **Subject privacy** (de‑identification, session scope).
- **Role‑based access control** (who can see what; who can direct).
- Future regulatory/compliance constraints.

### 2. Role‑based access

Define:

```ts
type UserRole = "admin" | "researcher" | "participant" | "viewer";
```

- Only `admin` / `researcher` can:
  - Create collab sessions for a given `session_id`.
  - Act as `director` for that collab.
- `participant`:
  - Can join only collab sessions they are explicitly invited to.
  - Only see **their own** subject MPG / signals (if sessions are per‑subject).
- `viewer`:
  - Read‑only, passive; can watch tours or live scenes but not control them.

### 3. Subject anonymization

- All participant‑visible labels in Vision Pro & web scenes:
  - Use pseudonymized `subject_id` (e.g., “P‑07”) and avoid raw names/identifiers.
- MPG nodes:
  - Text in `reasoning_provenance` and `evidence_preview` is **sanitized**:
    - Remove personal names, addresses, etc.
    - Optionally replaced with neutral placeholders.

### 4. Session scoping

- Collab sessions **bind** to exactly one underlying `session_id`:
  - No cross‑subject cross‑session mixing in the same collab.
- Tours (CR‑414) are scoped to that same session and cannot be reused across different subjects unless explicitly duplicated/edited.

### 5. Audit logs

- For each collab session:
  - Log who joined / left, who was director when, and which tours were played.
  - For director commands:
    - Log key changes (mode switches, focus on specific RV/MUFS/decision).
- Store logs in an append‑only table for later review if needed.

### 6. Acceptance criteria

- Only authorized users can start director sessions and tours.
- Subjects/participants never see identifying information about other subjects.
- There is a clear, reviewable trail of who controlled what during collaborative explorations.

---

If you’d like, the next iteration could go even more “sci‑fi practical” with **CR‑416+**:  

- A **multi‑subject “coherence wall”** where you can see several H3LIX brains at once (for group experiments / collective coherence).  [oai_citation:3‡Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  
- Or a **teaching/demo mode** where the app auto‑generates simplified educational tours of SORK‑N, Rogue Variables, and MUFS for students.
