**Change Request: CR‑007 – Policy Learning over (Collective) MPG for Safe, Auditable Interventions**

---

## 1. Objective

Build a **policy‑learning layer** on top of the individual + collective MPG stack (CR‑001–CR‑006) that:

1. Uses **Rogue Variables + Potency + coherence** to select *where* to intervene (which segments / pathways, individual or group).  [oai_citation:0‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
2. Learns *which intervention* (e.g., what to surface, who to nudge, when to slow/stop) improves outcomes and restores coherence, framed as a **contextual bandit / RL** problem.  
3. Keeps all interventions **auditable and governed**, consistent with the “functional role of RVs” (early warning, prioritization, targeted action, learning & re‑coherence, governance and impact accounting).  [oai_citation:1‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

This CR turns your architecture from *diagnosing* rogue structures into **acting on them adaptively and safely**.

---

## 2. Scope

### In scope

- Neo4j schema for **interventions & policies**:
  - `:InterventionType`, `:Policy`, `:PolicyEpisode`, `:PolicyOutcome`.
- Python components:
  - A **policy environment** wrapping Mirror Core, RVs, coherence, and outcomes.
  - A **contextual bandit** implementation (e.g., linear UCB or Thompson sampling), inspired by index‑style prioritization mentioned in the RV section.  [oai_citation:2‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
  - Safety & governance filters (allowed actions, intensity caps, audit trails).
- API endpoints for:
  - Getting **recommended interventions** for a trial / session.
  - Logging human overrides & feedback.

### Out of scope

- Complex deep RL (we start with contextual bandits / simple RL).
- Rich UI/UX for planning or editing policies (API only).
- Hard guarantees about human mental health impact (we keep interventions conservative & reversible and recommend IRB/ethics review for real users).

---

## 3. Theoretical grounding

From the paper:

- RVs are structural patterns in MPG with outsized explanatory power; Potency Index ranks their **future impact**.  [oai_citation:3‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
- Impact factors include RoC, BoI, Amplification, Affective load, Gate leverage, Robustness; these are exactly what we need to prioritize **where to act**.  [oai_citation:4‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
- RVs’ functional role includes **Early warning, Prioritization, Targeted action, Learning and re‑coherence, Governance and impact accounting**.  [oai_citation:5‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
- The SORK‑N loop lets Noetic feedback change attention, thresholds, cadence, learning rates (N → S′ step).  [oai_citation:6‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

CR‑007 sits exactly here: it learns **how N should modulate S′** in the presence of particular RVs/coherence states, under explicit safety constraints.

---

## 4. Schema: interventions & policies in Neo4j

### 4.1 Interventions

New label: `:InterventionType` – *what* can be done.

Examples:

- `SLOW_DECISION` – increase minimum decision time.
- `REQUEST_JUSTIFICATION` – ask user/system to articulate reasons.
- `SURFACE_EVIDENCE` – show specific evidence tied to a segment.
- `DEFOCUS_SEGMENT` – down‑weight a high‑potency path temporarily.
- `FOCUS_SEGMENT` – highlight a segment as a cue.
- `TAKE_BREAK` – suggest pause / break.
- `SOFT_ALERT` – “you might be biased toward X”.

Properties:

```text
id: string
name: string
layer_target: "Somatic" | "Symbolic" | "Noetic" | "Collective"
risk_level: "LOW" | "MEDIUM"  // no high-risk actions in v1
description: string
parameters_schema: map        // JSON schema / hints for params
active: bool                  // can be turned off if misbehaves
```

### 4.2 Policies & episodes

New labels:

- `:Policy` – a learned mapping from context → intervention.
- `:PolicyEpisode` – one decision of the policy (bandit arm pull).
- `:PolicyOutcome` – measured results for that episode.

Constraints:

```cypher
CREATE CONSTRAINT policy_id IF NOT EXISTS
FOR (p:Policy) REQUIRE p.id IS UNIQUE;

CREATE CONSTRAINT policy_episode_id IF NOT EXISTS
FOR (e:PolicyEpisode) REQUIRE e.id IS UNIQUE;

CREATE CONSTRAINT policy_outcome_id IF NOT EXISTS
FOR (o:PolicyOutcome) REQUIRE o.id IS UNIQUE;
```

Relationships:

```text
(:Policy)-[:USES_INTERVENTION]->(:InterventionType)
(:Policy)-[:RUNS_ON_GROUP]->(:Group)          // optional
(:Policy)-[:RUNS_ON_PARTICIPANT]->(:Participant)

(:Policy)-[:HAS_EPISODE]->(:PolicyEpisode)
(:PolicyEpisode)-[:FOR_TRIAL]->(:Trial)
(:PolicyEpisode)-[:APPLIED_INTERVENTION]->(:InterventionType)
(:PolicyEpisode)-[:HAS_OUTCOME]->(:PolicyOutcome)
```

`PolicyEpisode` properties:

```text
id: string
policy_id: string
trial_id: string
timestamp: datetime

context_hash: string          // hashed feature vector
rv_segment_ids: [string]      // segments that triggered policy
collective_segment_ids: [string]

chosen_intervention_id: string
parameters: map               // actual param values

human_override: bool
override_type: string | null
```

`PolicyOutcome` properties:

```text
id: string
episode_id: string
reward: float                 // scalar reward used for bandit/RL
delta_coherence: float        // C_after - C_before
delta_accuracy: float         // change vs baseline
delta_rt: float               // change in response time
notes: string
```

---

## 5. Policy environment & reward shaping

We wrap Mirror Core + Noetic + RV stack into an RL‑style **environment**.

### 5.1 Context vector

For each trial (individual or group) we build a **context vector** x:

- Individual / group indicators.
- Top‑K active segments and/or CollectiveSegments:
  - Potency, rv_score, affective load, gate leverage, robustness.
- Noetic metrics:
  - coherence C(t),
  - entropy‑change indicators.
- Task metadata:
  - difficulty band,
  - awareness condition (FULL/IU/PU/MIX),
  - recent flip history, etc.

This aligns with the paper’s idea that RVs carry a “passport” of properties and that Noetic coherence controls thresholds, attention, cadence.  [oai_citation:7‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

### 5.2 Action space

Each **action** is:

```text
(action_type, parameters)  where action_type is an InterventionType
```

Examples:

- `("SLOW_DECISION", {"extra_ms": 400})`
- `("REQUEST_JUSTIFICATION", {"length": "short"})`
- `("SURFACE_EVIDENCE", {"segment_id": S, "max_items": 3})`
- `("FOCUS_SEGMENT", {"segment_id": S})`
- `("DEFOCUS_SEGMENT", {"segment_id": S})`

We keep the discrete action set small initially (5–10 actions), each with small, safe parameter ranges.

### 5.3 Reward

Reward design must be **bounded, conservative, and interpretable**:

- Let:

  - `ΔC = C_after - C_before` (Noetic coherence change)  
  - `Δacc = acc_after - baseline_acc` (e.g. vs group or past block)  
  - `Δcal = −|calibration_error_after| + |calibration_error_before|`  

- Example scalar reward:

  ```text
  reward = w_c * ΔC + w_acc * Δacc + w_cal * Δcal − w_rt * (Δrt / baseline_rt)
  ```

- Cap reward to [-1, 1] and log all components in `:PolicyOutcome` for audit.

This directly implements the paper’s goal: test if coherent states (and the interventions that promote them) **preserve or improve accuracy & calibration under constraints**.  [oai_citation:8‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

---

## 6. Learning algorithm: contextual bandits

We implement a **contextual bandit** instead of full RL at first, matching the “index policy” flavor referenced in the RV section.  [oai_citation:9‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

### 6.1 Python Policy class

Sketch:

```python
# policies/contextual_bandit.py
import numpy as np
from dataclasses import dataclass
from typing import List, Dict, Any

@dataclass
class ActionDef:
    id: str                      # InterventionType.id
    name: str
    params_template: Dict[str, Any]

class LinearUCBBandit:
    def __init__(self, actions: List[ActionDef], d: int, alpha: float = 1.0):
        self.actions = actions
        self.d = d              # context dimension
        self.alpha = alpha

        # A_a = I_d, b_a = 0_d for each action
        self.A = {a.id: np.eye(d) for a in actions}
        self.b = {a.id: np.zeros((d, 1)) for a in actions}

    def _theta(self, aid: str) -> np.ndarray:
        A_inv = np.linalg.inv(self.A[aid])
        return A_inv @ self.b[aid]

    def select_action(self, x: np.ndarray) -> ActionDef:
        # x shape: (d, )
        x_vec = x.reshape(-1, 1)
        best_val = -np.inf
        best_action = self.actions[0]
        for a in self.actions:
            A_inv = np.linalg.inv(self.A[a.id])
            theta = self._theta(a.id)
            mu = float(theta.T @ x_vec)
            ucb = self.alpha * float(np.sqrt(x_vec.T @ A_inv @ x_vec))
            val = mu + ucb
            if val > best_val:
                best_val = val
                best_action = a
        return best_action

    def update(self, x: np.ndarray, aid: str, reward: float):
        x_vec = x.reshape(-1, 1)
        self.A[aid] += x_vec @ x_vec.T
        self.b[aid] += reward * x_vec
```

This gives you:

- `select_action(x)` – choose intervention for a given context.  
- `update(x, aid, reward)` – update from observed reward.

### 6.2 Neo4j integration

We create helper functions to:

- Serialize context (or its hash) + action + reward into `PolicyEpisode` and `PolicyOutcome`.
- Load past episodes to re‑train bandits across restarts.

---

## 7. Safety & governance

Directly aligned with the “Governance and impact accounting” role of RVs.  [oai_citation:10‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

### 7.1 Allowed actions & risk tiers

- Only allow **low‑risk, reversible interventions** (UI prompts, timing changes, surfacing information).
- Hard‑code a whitelist of `InterventionType` ids for **learning**; everything else can only be manually triggered.
- For high affective load segments (Affective Potency high, from CR‑003), restrict to gentler actions (e.g. soft alerts, slower decision) – no aggressive nudging or exposure.

### 7.2 Human oversight

- Add `human_override` to `PolicyEpisode` whenever a human (or higher‑level policy) rejects or modifies the suggested intervention.
- Track override rate per policy & per action; if override rate > threshold, automatically **downgrade** that action (reduce prior, mark for review).

### 7.3 Governance dashboard

(Not code, but design):

- Show per‑policy:

  - Top actions by usage and reward.
  - Distribution of rewards & Δcoherence.
  - Override rates.
  - Which segments/CollectiveSegments are most often targeted.

This supports “versioned and narrated” changes to structure and policy.  [oai_citation:11‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

---

## 8. Mirror Core integration

Modify Mirror Core (from earlier CRs) to call the policy engine in the **N → S′** step:

```python
class MirrorCore:
    def __init__(self, somatic, symbolic, noetic, mpg_repo, policy_engine):
        ...

    def sorkn_step(self, trial_context):
        # S, O, R, K as before...
        coherence_before = self.noetic.get_coherence(trial_context)
        active_rvs = self.mpg_repo.get_active_rvs(trial_context)

        x = build_context_vector(trial_context, coherence_before, active_rvs)
        action_def = self.policy_engine.select_action(x)

        # Apply intervention (N → S′)
        outcome_signals = self.apply_intervention(action_def, trial_context)

        coherence_after = self.noetic.get_coherence(trial_context)
        reward = compute_reward(coherence_before, coherence_after, outcome_signals)

        self.policy_engine.update(x, action_def.id, reward)
        log_policy_episode_to_neo4j(...)
```

For **Collective MPG** (CR‑006), you can have a `GroupMirrorCore` using the same policy engine but with group contexts & actions (e.g., “ask person P to explain to group”, “highlight shared RV to team”).

---

## 9. API endpoints

Extend API with:

```python
# api/policy_api.py
from fastapi import APIRouter
from pydantic import BaseModel

router = APIRouter(prefix="/policy", tags=["policy"])

class PolicyContext(BaseModel):
    trial_id: str
    # optional hints / overrides
    allow_group: bool = False

@router.post("/recommend")
def recommend_intervention(ctx: PolicyContext):
    # Load context from DB, build x, call bandit.select_action
    # Return proposed intervention + rationale (R from MPG + policy stats)
    ...

class PolicyFeedback(BaseModel):
    episode_id: str
    reward: float
    human_override: bool = False
    override_type: str | None = None

@router.post("/feedback")
def policy_feedback(fb: PolicyFeedback):
    # Update bandit + PolicyOutcome in Neo4j
    ...
```

---

## 10. How to run CR‑007 end‑to‑end

1. **Schema updates**: create `:InterventionType`, `:Policy`, `:PolicyEpisode`, `:PolicyOutcome` constraints and relationships in Neo4j.  
2. Implement `LinearUCBBandit` (or similar) and a **policy engine** wrapper.  
3. Modify Mirror Core to:
   - Build context vectors from RV/Potency + coherence + task data.  
   - Call policy engine for action selection & update.  
   - Log all episodes & outcomes to Neo4j.  
4. Add FastAPI endpoints for recommendation + feedback.  
5. Run **shadow mode** first:
   - Policy suggests actions but they are not applied, only logged.  
   - Analyze hypothetical rewards & check safety.  
6. Then run **controlled online experiments** (e.g., small N trials, conservative interventions):
   - Compare baseline vs policy‑assisted blocks on:
     - coherence, accuracy, calibration, MUFS incidence.  

---

## 11. Acceptance criteria

CR‑007 is complete when:

1. **Policy schema** is in place and Neo4j holds `:InterventionType`, `:Policy`, `:PolicyEpisode`, `:PolicyOutcome` nodes linked to `:Trial` and segments.  
2. **Policy engine** (contextual bandit) can:
   - Ingest context vectors from Mirror Core (individual + group).  
   - Suggest interventions from a safe, whitelisted set.  
   - Update from scalar rewards.  
3. **N → S′ integration**:
   - Mirror Core uses the policy engine in live SORK‑N trials, writing all episodes + outcomes back to Neo4j.  [oai_citation:12‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
4. **Governance & safety**:
   - Human overrides are tracked; override rates per action can be queried.  
   - No high‑risk interventions are autonomously chosen; actions are reversible and bounded.  
5. **Empirical check (pilot)**:
   - On at least one toy or pilot task, policy‑enabled runs show:
     - improved or equal accuracy vs baseline,  
     - improved coherence or calibration in at least some conditions,  
     - no obvious pathological behavior.  
6. **Documentation**:
   - CR‑007 is written up as `docs/CR-007-Policy-Learning-and-Interventions.md`, including:
     - action definitions,
     - reward shaping choices,
     - safety constraints,
     - known limitations.

If you want to keep going, a natural **CR‑008** would be about **meta‑policies & versioning**: learning *when to trust* a given policy, automatically rolling back or branching policy versions when behavior drifts, and integrating policy selection itself into the Noetic coherence layer.
