**Change Request: CR‑102 – Expanded KMP Edge App (Multisensor, Rich Tasks, Offline Experiments)**  

---

## 1. Objective

Extend CR‑101’s minimal KMP app into a **full H3LIX edge node** that can:

1. Collect **multiple somatic streams** (HR/HRV, steps, motion, respiration/EDA where possible) and basic behavioral context, in line with the Somatic layer’s signal catalog.  [oai_citation:0‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
2. Run **MPG‑Intuition tasks natively on device** (stimulus presentation, full vs restricted awareness, self‑report, awareness checks).  [oai_citation:1‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
3. Support **offline experiments** with robust local buffering and replay into the LAIZA backend when connectivity is available.

CR‑102 is about making the KMP app a **portable micro‑LAIZA lab**.

---

## 2. Scope

### In scope

- KMP shared module enhancements:
  - Richer **sensor abstractions** (HR/HRV, steps, sleep, motion, optional SpO₂/respiration/EDA if exposed by OS or device SDK).  [oai_citation:2‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
  - Local **experiment engine** for MPG‑Intuition tasks (trial definitions, timing, randomization).
  - Offline **event queue** with back‑pressure & prioritization.
- Android & iOS apps:
  - UX for **scope‑granted sensor selection**, experiment enrollment, and self‑report flows (intuition & awareness per CR‑005).  [oai_citation:3‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
  - Visual masking / simple psychophysical manipulations to induce **Input Unawareness (IU)** on‑device.  [oai_citation:4‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

### Out of scope

- Non‑mobile platforms (desktop, web) – those can come later.  
- Deep on‑device ML (we keep local computations simple; heavy inference stays server‑side).

---

## 3. KMP shared module extensions

### 3.1 Sensor abstraction layer

Create interfaces in `shared`:

```kotlin
interface SomaticSource {
    val id: String           // e.g. "hr", "steps", "accel"
    suspend fun start()
    suspend fun stop()
    val events: Flow<EventEnvelope>
}
```

Concrete platform modules (`androidApp`, `iosApp`) implement:

- `HeartRateSource`
- `StepsSource`
- `MotionSource` (accelerometer/gyroscope)  
- Optional: `RespirationSource`, `EdaSource` when OS/device supports them.

The **shared** layer then composes:

```kotlin
class SomaticManager(
    private val sources: List<SomaticSource>
) {
    fun events(): Flow<EventEnvelope> =
        sources
            .map { it.events }
            .merge()        // kotlinx.coroutines Flow.merge
}
```

All sources emit **EventEnvelope** instances defined in CR‑101.

### 3.2 Experiment engine

Model a trial config that mirrors the MPG‑Intuition setup (Decision set, stimuli, awareness condition, IU/PU flags).  [oai_citation:5‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

```kotlin
@Serializable
data class MobileTrialConfig(
    val trialId: String,
    val stimulusType: String,          // "image", "text", "audio"
    val stimulusPayload: String,       // URL or inline resource ID
    val awarenessCondition: String,    // "FULL" | "IU" | "PU" | "MIX"
    val maskType: String?,             // "BRIEF_MASK", "PERIPHERAL", "NONE"
    val decisionOptions: List<String>, // e.g. ["A", "B"]
    val itiMs: Long                    // inter-trial interval
)
```

Experiment engine responsibilities:

- Load `List<MobileTrialConfig>` from backend or bundled JSON.
- Randomize order per session with reproducible seed.
- Run per‑trial lifecycle:
  1. Pre‑trial baseline (collect Somatic for N ms).  [oai_citation:6‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
  2. Show stimulus (full or masked).  
  3. Collect decision + response time.  
  4. Collect **intuition/self‑report & confidence**.  [oai_citation:7‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
  5. Optional on‑device awareness check (IU verification).  [oai_citation:8‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
  6. Emit **`task` events** with all metadata to backend.

Each trial is also mirrored in Neo4j as `:Trial` (CR‑005) once uploaded.

### 3.3 Offline queue + replay

In `shared`:

- Use SQLDelight to store a local queue table:

```sql
CREATE TABLE PendingEvent (
  id TEXT PRIMARY KEY,
  payload TEXT NOT NULL,
  createdAt INTEGER NOT NULL,
  priority INTEGER NOT NULL
);
```

- A **QueueManager**:

```kotlin
class QueueManager(
    private val dao: PendingEventDao,
    private val networkClient: EventClient
) {
    suspend fun enqueue(event: EventEnvelope, priority: Int = 1)

    suspend fun flush() {
        val batch = dao.loadOldest(limit = 100)
        if (batch.isEmpty()) return

        try {
            networkClient.send(batch.map { it.toEventEnvelope() })
            dao.delete(batch.map { it.id })
        } catch (e: IOException) {
            // keep; exponential backoff controlled elsewhere
        }
    }
}
```

- Flush triggered by:
  - connectivity changes,  
  - app foreground events,  
  - periodic background worker (WorkManager / BGTaskScheduler).

This guarantees experiments can run fully **offline** in the field; events replay later into LAIZA.

---

## 4. Android specifics (examples)

### 4.1 Health Connect (HR/HRV, steps, sleep)

- Integrate with **Health Connect** (Android 13+):
  - Request permissions for HR, steps, sleep sessions.  
  - Poll or subscribe (depending on API) every X seconds/minutes.  
  - Convert samples → `somatic` EventEnvelope.

HR/HRV sample → envelope:

```kotlin
payload = buildJsonObject {
    put("bpm", sample.beatsPerMinute)
    put("hrvRmssdMs", sample.rmssd ?: JsonNull)
    put("origin", "health_connect")
}
```

### 4.2 Motion & simple respiration proxy

- Use accelerometer & gyroscope to compute:
  - motion energy (norm of acceleration),  
  - approximate breathing patterns (chest phone scenario only, with care).

Emit as `somatic` events with appropriate `quality` metadata (lower trust vs direct respiration sensors).

### 4.3 IU masking implementation

For visual tasks:

- Show stimulus image for `X` ms (e.g., 16–33 ms).
- Immediately replace with **mask** (high‑contrast noise or irrelevant image) for `Y` ms, as standard psychophysics for brief masking.  [oai_citation:9‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
- For peripheral micro‑cues, present small cue near edges of screen, away from fixation.

All presentation timing uses `Choreographer` / `Handler` with careful measurement of actual durations.

---

## 5. iOS specifics (examples)

- Use **HealthKit**:
  - HKQuantityTypeIdentifier.heartRate, stepCount, etc.
- Use **CoreMotion** for accelerometer/gyro.

Platform differences are encapsulated behind `SomaticSource` implementations; shared logic stays identical.

---

## 6. Backend adjustments (for CR‑102)

- Extend Ingest Gateway schemas to accept richer `somatic` payloads (`hrvRmssdMs`, `steps`, `motionEnergy`, etc.).  [oai_citation:10‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
- Add APIs for:
  - **Downloading trial configs**: `/api/mobile/experiments/{id}` returning `List<MobileTrialConfig>`.  
  - **Registering sessions** started on device → create `:Session` in Neo4j (CR‑005).  [oai_citation:11‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

---

## 7. Acceptance criteria for CR‑102

CR‑102 is done when:

1. The KMP app **collects multi‑sensor somatic data** (HR, steps, motion) and pushes them via EventEnvelope → backend → Neo4j.  
2. At least one **MPG‑Intuition task** runs fully on device (full vs masked, decision & self‑report), producing proper `:Trial` + `:SelfReport` + `:AwarenessCheck` nodes.  [oai_citation:12‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
3. Experiments work **offline**: if you disable network, run a block, then re‑enable, all events replay and appear in backend.  
4. Time‑locking is verified: somatic and task events for a trial align to within expected jitter.  
5. Consent/scope settings allow opt‑in/out per sensor, stored and respected.

---

---

**Change Request: CR‑103 – Full External Connectors (Email/Chat/Audio/Video) into MPG & MUFS Pipelines**

---

## 1. Objective

Extend CR‑101/102 so the system can ingest **real‑world digital traces**:

- Email, messaging, calendar/doc activity (Symbolic layer).  [oai_citation:13‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
- Audio and video recordings (Somatic + Symbolic).  [oai_citation:14‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

and map them into:

- **MPG nodes/segments & Evidence** (Layer‑Type Graph),  
- **SegmentState** histories and Noetic coherence,  
- MUFS, RV & Potency pipelines from earlier CRs.

This makes LAIZA a genuinely **always‑on, life‑wide protocol**, not just a lab‑task engine.  [oai_citation:15‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

---

## 2. Scope

### In scope (v1)

- Microservices:
  - `email_connector` (Gmail/IMAP).
  - `chat_connector` (e.g., Slack/Discord demo).
  - `calendar_connector` (events & meetings).
  - `media_connector` (ingests audio/video file metadata; delegates heavy processing).
- Processing pipelines:
  - **Text pipeline** → Symbolic layer → MPG updates.  [oai_citation:16‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
  - **Audio pipeline** → ASR → text; prosody → somatic features.  [oai_citation:17‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
  - **Video pipeline** → pose/face features (somatic); optional scene context (symbolic).
- Event mapping:
  - Everything to **EventEnvelope** and through same streaming backbone as CR‑101.

### Out of scope (v1)

- Every platform under the sun (we pick *one* email and *one* chat provider).  
- Client‑side connectors (these are backend/cloud‑side services).  
- Heavy ML training (assume you call out to existing ASR/vision models or hosted services).

---

## 3. Email connector

### 3.1 Service behavior

`email_connector`:

- For each Participant with email scope:
  - OAuth login to Gmail (or generic IMAP with app password).
  - Periodic polling or push notifications for **new / changed messages**.
- For each message:
  - Extract:
    - `from`, `to`, `cc`, `subject`, `snippet`, `folders` (Inbox, Starred, etc.).
    - normalized timestamp.
  - Optionally store full body in encrypted object store; do **not** store in Neo4j verbatim.

### 3.2 Event mapping

Transform each email into an EventEnvelope:

```json
{
  "source": "gmail",
  "stream_type": "text",
  "payload": {
    "message_id": "...",
    "direction": "IN" | "OUT",
    "subject": "...",
    "snippet": "...",
    "folder": "INBOX",
    "thread_id": "...",
    "participants": ["alice@example.com", "bob@example.com"]
  }
}
```

`participant_id` comes from a secure mapping of email → Participant (not in Neo4j).  

These events hit the **Ingest Gateway**, then `events.text` topic.

---

## 4. Chat & calendar connectors

### 4.1 Chat

`chat_connector` for Slack/Discord:

- Use bot/user tokens to listen to message events in specified channels/DMs.
- Map each message → EventEnvelope (`stream_type="text"`, `source="slack"`, payload with channel, thread, participants, message text).

### 4.2 Calendar

`calendar_connector`:

- Connect via Google Calendar / Outlook API.
- For each event:
  - Start/end, title, location, participants, description.
- Emit as `stream_type="meta"`; these form **contextual anchors** for MPG segments (projects, recurring events, relationships).

---

## 5. Text → MPG processing (Symbolic)

The **Symbolic Processor** from CR‑101/102 is extended:

1. **Pre‑processing**:
   - Deduplicate / thread messages (emails & chats).
   - Basic classification: topic, sentiment, “task/project” detection.

2. **LLM/NLP mapping** (Symbolic layer per Sec. 3.2):  [oai_citation:18‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
   - Extract **entities** (people, orgs, projects), **events** (meetings, conflicts, decisions), **relations** (depends‑on, supports, contradicts).  
   - Assign layers (Psychological, Social, Professional, etc.) to nodes via `λ : V → P(L)` as in Definition 1.  [oai_citation:19‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

3. **Update MPG**:
   - For each identified construct, ensure a `:MPGNode` exists (or create).  
   - Add edges per relation types (`causes`, `triggers`, `contradicts`, etc.).  [oai_citation:20‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
   - Attach `:Evidence` records containing:
     - pointer to message/meeting,
     - text snippet (lightly truncated),
     - per‑item support `ci`, quality multiplier `qi`, diversity `ui`, timeliness `ti` → computed **Conf** via Eq. (1).  [oai_citation:21‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

4. **Segmentation & Lift** (Def. 2–4):  [oai_citation:22‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
   - Periodically run segmentation over updated graph:
     - segments for projects, relationships, stressors, “contradiction clusters” similar to Fig. 3.  [oai_citation:23‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
   - Apply `Lift(G^(k))` to update higher‑level segments and propagate importance/confidence upward and downward.

This makes long‑running communication patterns show up as cohesive segments and routes in the MPG.

---

## 6. Audio & video ingestion

### 6.1 Media connector

`media_connector`:

- Receives **file upload events** from:
  - KMP app (recorded diary, lab sessions),
  - integrated video platforms (Zoom/Meet recordings via API),
  - local lab capture (e.g., cameras in experimental setup).
- Stores raw files in secure object storage; emits `audio` or `video` EventEnvelope with:

```json
"payload": {
  "uri": "s3://bucket/p123/audio/seg_001.wav",
  "duration_ms": 59000,
  "session_id": "S456",
  "experiment_tag": "E1" 
}
```

### 6.2 Audio pipeline

`audio_processor`:

1. Fetch audio from URI.  
2. Run ASR → text:
   - Emit derived `text` events back into `events.text` for Symbolic Processor.  
3. Extract prosody features:
   - pitch contours, energy, speaking rate, pauses → Somatic feature vectors (arousal, stress) as per Somatic layer spec.  [oai_citation:24‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
4. Emit **SomaticSummary** events & Evidence as in CR‑101.

### 6.3 Video pipeline

`video_processor`:

1. Fetch video.  
2. Run:
   - Pose estimation / motion tracking → micro‑motion, posture metrics.  
   - Facial expression model → arousal/valence proxies.  [oai_citation:25‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
3. Summarize into Somatic features (movement energy, gesture frequency, etc.).  
4. Optional: high‑level symbolic context (e.g., “meeting”, “outdoors”) → MPG node evidence.

Again, only **features + URIs**; no raw frames in Neo4j.

---

## 7. Feeding MUFS, RV & Potency

Once these external streams are mapped into MPG:

- RV pipeline (CR‑002/003) can now identify **Rogue Variables** like:
  - recurrent conflicts in particular email threads (segments with high RV Potency),  [oai_citation:26‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
  - high‑stress periods associated with specific projects (Somatic + Symbolic evidence for segments).  
- SegmentState temporal snapshots (CR‑003) will be enriched with:
  - evidence weight changes derived from communication volume, sentiment, etc.,  
  - coherence differences during meetings vs solo work.  [oai_citation:27‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
- MUFS search (CR‑004) can examine:
  - what happens to decisions if we **mask** particular Symbolic inputs (e.g., hide emotional tone, hide certain project nodes) or structural pathways derived from email/chat.

This is how “real life” feeds into MPG‑Intuition and RV analyses.

---

## 8. Privacy & consent specifics for CR‑103

- **Per‑source opt‑in** (email, chat, calendar, audio/video) with clear UI in KMP app and/or web console.  
- Token & credential handling:
  - All OAuth tokens stored in secure secrets manager, never in Neo4j.  
- Content minimization:
  - Only **snippets** and **derived features** in Neo4j; full contents in encrypted object store with access controls.  
- Ability to **pause or revoke** sources at any time; connectors listen to updates and stop fetching.

This directly addresses digital phenotyping safety considerations mentioned in the LAIZA description.  [oai_citation:28‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

---

## 9. Acceptance criteria for CR‑103

CR‑103 is done when:

1. **Email connector**:
   - Can ingest real messages for a test Participant (with consent) and create corresponding `text` events and MPG updates (nodes, edges, Evidence).  
2. **Chat & calendar connectors**:
   - At least one chat provider and calendar provider integrated in the same pattern, populating MPG with social/professional segments.  
3. **Audio pipeline**:
   - Can process at least one recorded session, producing:
     - ASR‑based Symbolic content,
     - prosody‑based Somatic summaries,
     - coherence updates via Noetic layer.  [oai_citation:29‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
4. **Video pipeline**:
   - Can process at least a short demo video into pose/face features and Somatic Evidence.  
5. **End‑to‑end MPG impact**:
   - Neo4j shows:
     - new segments / pathways clearly derived from real email/chat/media traces,  
     - RV & Potency scores for some segments,
     - these segments influencing MUFS search or being visible in trial contexts.  
6. **Privacy**:
   - Scopes & revocation flows implemented,
   - no raw email/chat bodies or AV frames stored directly in Neo4j,
   - secure object storage used for originals.

---

If you’d like, we can keep going with **CR‑104** as a “personal cockpit” app: a small, privacy‑first UI (desktop/web) that lets a participant **see** parts of their own MPG, RVs, and intuition episodes, approve/deny interventions, and fine‑tune what the system is allowed to look at or learn from.
