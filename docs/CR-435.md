The numbering here is conflicting. Please change it and add 10 so it starts with 435
Perfect, teaching mode it is üòä  
Per your request I‚Äôll label this as **CR‚Äë435** (teaching / curriculum mode) to avoid collisions with the previous batch.

---

## CR‚Äë435 ‚Äì Teaching / Curriculum Mode for H3LIX & LAIZA

**Status:** Proposed  
**Type:** UX / Content / Architecture  
**Baseline:**

- H3LIX architecture (Somatic, Symbolic, Noetic, Mirror Core, SORK‚ÄëN).  [oai_citation:0‚Ä°Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  
- MPG + Rogue Variables + MUFS + MPG‚ÄëIntuition model.  [oai_citation:1‚Ä°Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  
- Visualizations:  
  - Triple helix + Noetic halo + SORK‚ÄëN ring.  
  - MPG ‚Äúcity‚Äù + Rogue / MUFS overlays.  
  - Coherence Wall and MPG Galaxy.

This CR adds a **structured teaching mode** that turns all of this into:

- Short, guided lessons (‚Äútours‚Äù) with simplified data.  
- Built‚Äëin explanations, quizzes, and interactive exercises.  
- Reusable across **web** and **Vision Pro**.

---

### 1. Goals

1. Help **new users / students** understand:
   - H3LIX layers & SORK‚ÄëN loop.
   - MPG & segments / lift operator.  [oai_citation:2‚Ä°Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  
   - Rogue Variables & MUFS (and MPG‚ÄëIntuition).
   - Group coherence / echoes (Wall + Galaxy) at a high level.
2. Provide a **standard curriculum** for workshops and demos.
3. Reuse existing infrastructure:
   - **SceneControlState + tours** (CR‚Äë414).  
   - Coherence Wall & MPG Galaxy (CR‚Äë416+).  
   - Same web & Vision Pro clients.

Teaching mode = **pre‚Äëauthored tours + interactive checkpoints**, not free exploration.

---

### 2. Concept: Lesson, Module & Step

We introduce three levels:

```ts
interface LessonStep {
  step_id: string;
  title: string;
  description_md: string; // markdown description / script
  scene_control_delta: Partial<SceneControlState>;
  camera_hint?: CameraHint; // for web; optional for Vision Pro
  interaction_task?: InteractionTaskSpec;
  quiz?: QuizSpec;
  estimated_seconds?: number;
}

interface LessonModule {
  module_id: string;
  title: string;
  description_md: string;
  order: number;
  steps: LessonStep[];
}

interface Lesson {
  lesson_id: string;
  title: string;
  description_md: string;
  difficulty: "intro" | "intermediate" | "advanced";
  target_audience: "student" | "researcher" | "stakeholder";
  modules: LessonModule[];
  backing_data: LessonDataSource;
}
```

**LessonDataSource**:

```ts
type LessonDataSource =
  | { kind: "synthetic"; scenario_id: string } // curated toy data
  | { kind: "recorded_session"; session_id: string; window_ms: [number, number] }
  | { kind: "cohort"; cohort_id: string }; // for Coherence Wall / Galaxy intros
```

Each step:

- Applies a **SceneControlState delta** (time, focus, mode).
- Optionally defines:
  - An **interaction task** (e.g. ‚Äúfind the Rogue Variable‚Äù, ‚Äúscrub to see change‚Äù).
  - A **quiz** (MCQ, true/false, hotspot click).

---

### 3. Curriculum outline

Baseline curriculum (can be extended):

1. **Module A ‚Äì H3LIX & SORK‚ÄëN basics**
   - A1: The three layers (Somatic / Symbolic / Noetic) and triple helix.  [oai_citation:3‚Ä°Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  
   - A2: SORK‚ÄëN loop (Stimulus ‚Üí Organism ‚Üí Response ‚Üí Kontingenz ‚Üí Noetic ‚Üí S‚Ä≤).
   - A3: Simple decision cycle with clearly visible somatic spike and response outcome.

2. **Module B ‚Äì MPG architecture**
   - B1: Flat MPG graph: nodes, edges, labels, metrics m(v), Conf, Imp.  [oai_citation:4‚Ä°Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  
   - B2: Segments, interfaces, lift operator ‚Üí higher levels. (Use toy MPG like Fig. 1‚Äì3.)  [oai_citation:5‚Ä°Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  
   - B3: Time‚Äëindexed MPG: small update sequence (‚Äúlocalize, roll up, optionally restructure, project down‚Äù).  [oai_citation:6‚Ä°Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  

3. **Module C ‚Äì Rogue Variables**
   - C1: Intuition for Shapley contribution & 3‚Äësigma rule.  [oai_citation:7‚Ä°Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  
   - C2: RV as ‚Äúrogue segment‚Äù in a toy MPG with exaggerated prediction error.
   - C3: Potency Index and impact factors.

4. **Module D ‚Äì MUFS & MPG‚ÄëIntuition**
   - D1: Two versions of the same decision: full vs masked; show flip.  [oai_citation:8‚Ä°Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  
   - D2: Define MUFS; show minimal set in a small graph.  
   - D3: MPG‚ÄëIntuition: existence of nonempty MUFS; connect to Noetic coherence & self‚Äëreports.

5. **Module E ‚Äì Group coherence & MPG echoes (optional)**
   - E1: Simple Coherence Wall with 3‚Äì4 subjects, highlight high coherence region.  [oai_citation:9‚Ä°Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  
   - E2: MPG Galaxy: show structurally similar subjects clustering.

Each module should have:

- 3‚Äì7 steps.
- 10‚Äì20 minutes runtime at ‚Äúslow‚Äù pace.
- All content in markdown so it can be rendered in both web and Vision Pro HUDs.

---

### 4. Backend: Lesson & Teaching APIs

New endpoints:

```http
GET  /v1/lessons               # list all lessons
GET  /v1/lessons/{lesson_id}   # full lesson spec
POST /v1/lessons               # (admin) create/update lessons
```

Runtime progress per user:

```ts
interface LessonProgress {
  user_id: string;
  lesson_id: string;
  current_module_idx: number;
  current_step_idx: number;
  completed: boolean;
  quiz_results: QuizResult[];
}
```

Progress endpoints:

```http
GET  /v1/lessons/{lesson_id}/progress/{user_id}
POST /v1/lessons/{lesson_id}/progress/{user_id}
```

Implementation notes:

- Lessons can **reuse** `BrainTour` (CR‚Äë414) internally:
  - Each `LessonStep` is basically ‚Äútour segment + overlay UI‚Äù.
- Teaching mode uses **synthetic / curated data** by default to ensure clarity and privacy.

---

### 5. Teaching Mode UX ‚Äì Web

In the React app:

- Add a **‚ÄúLearn‚Äù tab**:
  - Shows available lessons (cards with title, difficulty, length).
  - Resume button uses `LessonProgress`.

Inside a lesson:

- Layout:

  - Left: **Script panel** (step title, text, images).
  - Center: 3D scene (helix / MPG / wall / galaxy) driven by `SceneControlState`.
  - Right: **Interaction / quiz panel** (if defined).

- Step nav:

  - Previous / next.
  - Progress bar across modules.

- Interaction tasks:

  Example types:

  ```ts
  type InteractionTaskSpec =
    | { kind: "select_node"; target_node_id: string }
    | { kind: "select_rv"; target_rogue_id: string }
    | { kind: "scrub_time"; from_ms: number; to_ms: number }
    | { kind: "toggle_mufs"; decision_id: string };
  ```

  - As learner performs the action (click node, toggle MUFS, scrub time interval), we mark step as ‚Äúcompleted‚Äù.

- Quiz types:

  ```ts
  type QuizSpec =
    | { kind: "mcq"; question: string; options: string[]; correct_index: number }
    | { kind: "true_false"; statement: string; answer: boolean }
    | { kind: "hotspot"; question: string; target_node_id: string };
  ```

  - Immediate feedback (correct/incorrect + short explanation) reinforcing the concept.

---

### 6. Teaching Mode UX ‚Äì Vision Pro

In the Vision Pro app:

- Add a **‚ÄúTeaching‚Äù mode** entry on the main window:

  - Select lesson + module.
  - Show progress.

- In ImmersiveSpace:

  - Brain / MPG / Wall / Galaxy as usual.
  - A **floating ‚Äúlesson card‚Äù HUD**:

    - Title & short text for current step.
    - Simple next/prev buttons.
    - Optional quiz overlay with big, gaze‚Äëfriendly buttons.

- Interaction:

  - Tasks map directly to gaze+pinch actions:
    - ‚ÄúSelect the Rogue Variable segment‚Äù ‚Üí highlight when correct; show explanation.
    - ‚ÄúScrub the time plane until the coherence wave calms‚Äù ‚Üí user grabs a handle and drags plane; we detect threshold crossing.
  - For quizzes:
    - 2‚Äì4 big answer tiles in space, at comfortable distance.

- Comfort:

  - Teaching mode is ‚Äúslow and smooth‚Äù:
    - Animations at gentle speeds.
    - No sudden camera moves; user moves their body/head to explore.

---

### 7. Content simplification & safety

Because H3LIX / LAIZA can involve sensitive physiological and narrative data, teaching mode should:

- Use **synthetic scenarios** by default:
  - Toy somatic patterns (clear spikes).
  - Stylized MPGs loosely inspired by the figures in the paper (e.g. small graph on *page 10* and the segments on *page 11*).  [oai_citation:10‚Ä°Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  
- When real data is used:
  - Ensure full anonymization + consent.
  - Option to **mask** reasoning_provenance texts to avoid personal details.

Pedagogically:

- Keep numbers small:
  - Very small MPGs (10‚Äì30 nodes) for initial lessons.
  - Single, clear Rogue Variable or MUFS per scenario.
- Explicitly note limitations (e.g., group coherence effects tentative, no metaphysical claims).

---

### 8. Authoring tools (advanced / optional)

For your own team to build future lessons:

- Simple JSON/YAML lesson format (mirroring `Lesson` structures).
- Internal web ‚ÄúLesson Editor‚Äù:
  - Load a session/cohort.
  - Scrub to a time, pick scene config.
  - Click ‚ÄúCapture as step‚Äù ‚Üí stores `SceneControlState` + camera hint.
  - Add text, tasks, and quiz items.

This leverages tour recording (CR‚Äë414) but adds **annotations & assessments** on top.

---

### 9. Acceptance criteria

- **Web**:
  - At least one fully implemented lesson (e.g. ‚ÄúIntro to H3LIX & SORK‚ÄëN‚Äù) that a new user can complete end‚Äëto‚Äëend without prior knowledge.
  - Interaction tasks and quizzes are tracked in `LessonProgress`.

- **Vision Pro**:
  - Same lesson playable in Teaching mode.
  - Text & UI legible and comfortable in ImmersiveSpace.

- **Content**:
  - Lessons correctly explain:
    - Three H3LIX layers + SORK‚ÄëN.  [oai_citation:11‚Ä°Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  
    - MPG, segments, lift operator.
    - Rogue Variables (Shapley, segments/pathways) and Potency.  [oai_citation:12‚Ä°Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  
    - MUFS and MPG‚ÄëIntuition.  [oai_citation:13‚Ä°Symbiotic_human_AI_architecture.pdf](sediment://file_00000000a2f071f4a36d5a4040b12b83)  

Once this CR is implemented, you essentially have a **built‚Äëin course** on your own architecture, running inside the same visualizations and tools you‚Äôll use for research and experiments‚Äîgreat for onboarding new collaborators, students, or stakeholders without drowning them in raw math and code.
