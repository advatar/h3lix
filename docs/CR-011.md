**Change Request: CR‑011 – External H3LIX/LAIZA Benchmark Hub & Collaborative MPG‑Intuition Leaderboard**  [oai_citation:0‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

---

## 1. Objective

Build an **external collaboration layer** on top of the H3LIX / LAIZA open‑science package (CR‑010) so that:

1. Other labs/teams can **download datasets & configs**, run the MPG‑Intuition experiments (E1–E3 from CR‑009), and **submit results**.  
2. There is a **central leaderboard + registry** of architectures tested under the *same* LAIZA protocol (full vs restricted awareness, MUFS, coherence, RVs).  [oai_citation:1‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
3. All submissions are **auditable, comparable, and ethically scoped** (synthetic or properly anonymized data only).

Think: “H3LIX Benchmark Hub” + a **MPG‑Intuition leaderboard** for system‑only and human‑like simulations.

---

## 2. Scope

### In scope

- A **benchmark spec** (schemas + metrics) for LAIZA/H3LIX experiments:
  - Core tasks: E1 MPG‑Intuition, E2 Coherence vs performance, E3 RV Potency ablation.  [oai_citation:2‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
- A **submission format** (metadata + metrics + optional per‑trial logs).  
- A small **evaluation service** that:
  - validates and scores submissions,  
  - stores them in a leaderboard DB (could be Neo4j or SQL).  
- Documentation for:
  - how to run local experiments (CR‑010 package),  
  - how to prepare and upload submissions,  
  - how to interpret leaderboard metrics.

### Out of scope

- Hosting real human data (only synthetic or rigorously anonymized demo data).  [oai_citation:3‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
- Complex web front‑end; we’ll assume a minimal REST API + simple HTML / notebook views are enough to start.

---

## 3. Benchmark & metrics spec

We standardize what other groups must report so comparisons are meaningful.

### 3.1 Task families (v1)

We use the experiment families derived from Section 5–6 of the paper:  [oai_citation:4‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

- **E1 – MPG‑Intuition & MUFS**  
  - Full vs restricted awareness (IU & PU).  
  - MUFS detection; MPG‑Intuition labeling.

- **E2 – Coherence vs performance**  
  - Coherence metrics (correlations, entropy change, coherence spectra) vs accuracy and calibration.  [oai_citation:5‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

- **E3 – RV & Potency ablations**  
  - Ablate top‑Potency segments vs random segments; compare impact.

Each task has:

- A **canonical dataset** (synthetic / anonymized).  
- An **evaluation config** (YAML) specifying:
  - trial count,  
  - awareness conditions,  
  - which metrics to report.

### 3.2 Required metrics (per condition)

Per experiment condition (e.g., “H3LIX_FULL_RESTRICTED”, “Baseline_NO_MPG”):

- `accuracy`  
- `log_loss` or `brier_score`  
- `has_mufs_rate` (fraction of trials with nonempty MUFS)  
- `mpg_intuitive_rate`  
- `coherence_mean`, `coherence_std`  
- `rv_count`, `rv_potency_topK_mean`  
- `delta_accuracy_ablation` (for E3)  
- `delta_coherence_ablation` (for E3)

Optional (encouraged):

- Per‑trial CSV with:
  - decision, correctness, confidence, coherence, MUFS size, Potency stats.

---

## 4. Submission format

Submissions are **zip bundles** with a simple manifest.

### 4.1 Directory layout

```text
submission_<team>_<date>.zip
  manifest.json
  results/
    E1_MPG_Intuition/
      C4_H3LIX_FULL_RESTRICTED_summary.json
      C4_H3LIX_FULL_RESTRICTED_trials.parquet  (optional)
    E2_Coherence/
      ...
  models/
    README_MODEL.md          # optional description of agent/architecture
  logs/
    run.log                  # for debugging
```

### 4.2 `manifest.json`

Example:

```json
{
  "team_id": "lab_xyz",
  "contact": "lab@xyz.edu",
  "architecture_name": "MyH3LIXVariant",
  "stack_description": "Somatic+Symbolic+Noetic+MPG+RV+MUFS, no policies",
  "experiments": [
    {
      "id": "E1_MPG_Intuition",
      "conditions": ["C0_BASELINE_FULL", "C4_H3LIX_FULL_RESTRICTED"],
      "config_hash": "sha256:abc123..."
    },
    {
      "id": "E2_Coherence",
      "conditions": ["C2_SOMATIC", "C4_H3LIX_FULL_RESTRICTED"]
    }
  ],
  "code_repo_url": "https://... (optional)",
  "notes": "Using our own somatic simulator; canonical datasets unchanged."
}
```

### 4.3 Summary JSON schema (per condition)

```json
{
  "experiment_id": "E1_MPG_Intuition",
  "condition_id": "C4_H3LIX_FULL_RESTRICTED",
  "n_trials": 500,
  "metrics": {
    "accuracy": 0.78,
    "log_loss": 0.55,
    "brier_score": 0.19,
    "has_mufs_rate": 0.62,
    "mpg_intuitive_rate": 0.58,
    "coherence_mean": 0.64,
    "coherence_std": 0.12,
    "rv_count": 5.3,
    "rv_potency_top5_mean": 0.81,
    "delta_accuracy_ablation": -0.12,
    "delta_coherence_ablation": -0.08
  },
  "random_seed": 123,
  "run_time_sec": 840
}
```

---

## 5. Evaluation service & leaderboard

### 5.1 Server responsibilities

A small **evaluation service** (FastAPI or similar) with endpoints:

- `POST /submit` – upload zipped submission.  
- `GET /leaderboard/{experiment_id}` – retrieve ranked results.  
- `GET /runs/{run_id}` – details for a specific run.

It will:

1. Validate JSON schemas.  
2. Check `config_hash` to ensure the **canonical configs** were used.  
3. Recompute some metrics from per‑trial logs (if provided) for spot checks.  
4. Insert records into a leaderboard DB.

### 5.2 Result storage schema

Could be SQL or Neo4j. Example node model in Neo4j:

- `:Team`  
- `:Method` (architecture/stack)  
- `:Experiment`, `:ExperimentCondition` (mirroring CR‑009)  
- `:SubmissionRun` (one zipped file)  
- `:MetricResult` (values)

Example relationships:

```text
(:Team)-[:SUBMITTED]->(:SubmissionRun)
(:SubmissionRun)-[:FOR_METHOD]->(:Method)
(:SubmissionRun)-[:FOR_CONDITION]->(:ExperimentCondition)
(:SubmissionRun)-[:HAS_METRIC]->(:MetricResult)
```

Leaderboard views:

- For each experiment/condition:
  - best accuracy,  
  - best calibration,  
  - interesting trade‑offs (e.g., high coherence but lower accuracy).

---

## 6. Interop with CR‑010

The CR‑010 “H3LIX Demo Lab” repo ships with:

- Canonical **datasets**,  
- Canonical **configs**,  
- **runner** that can create a submission bundle.

CR‑011:

- Adds `scripts/submit_to_hub.py`:
  - runs specified experiments locally,  
  - builds `submission_<team>.zip`,  
  - POSTs to `/submit`.  

- Documents how external teams can:
  1. Clone the repo or just pull the configs/datasets.  
  2. Plug in their own architecture (baseline, variant, etc.).  
  3. Run the runner against canonical configs.  
  4. Submit results.

---

## 7. Ethics & data constraints

Per the paper’s emphasis on **digital phenotyping safety and privacy**, and the need for **pre‑registration and cautious interpretation of noetic effects**:  [oai_citation:6‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

- **Leaderboard datasets are synthetic or strongly anonymized**.  
- Submitters **must not** upload real participant‑identifiable traces.  
- If teams run LAIZA on their own human data:
  - It stays within their lab; only aggregate metrics go into submissions.  
  - They confirm IRB/ethics approval and consent in the manifest’s `notes` field.

Clear guidance in `ETHICS_PRIVACY.md` explains:

- no raw neuro/physiology traces,  
- no direct self‑report text,  
- no linking of IDs that can identify real individuals.

---

## 8. Governance & versioning

We define:

- A small **Steering group** (you + collaborators) as maintainers.  
- Versioned **benchmark releases**:
  - `v1.0` – current experiments E1–E3 on synthetic datasets.  
  - `vX.Y` – new tasks or datasets; metrics version indicated in manifest.

Policy:

- Old results stay visible but are tagged with `benchmark_version`.  
- Any change to datasets or configs increments the benchmark version and gets a changelog entry in `EXPERIMENTS.md`.

---

## 9. How to run CR‑011 in practice

1. Extend the CR‑010 repo with:
   - `submission/` directory,  
   - `scripts/submit_to_hub.py`,  
   - validation schemas (e.g., `schemas/*.json`).  

2. Implement a small **benchmark hub service**:
   - Accepts and validates submissions.  
   - Stores metrics in DB and exposes `/leaderboard/*` endpoints.

3. Write `docs/BENCHMARK_HUB.md` explaining:
   - How to run a local experiment,  
   - How to build a submission zip,  
   - How to interpret leaderboard outputs.

4. Dry‑run:
   - Use your own H3LIX stack and a couple of baselines to populate initial leaderboard entries.  
   - Ensure re‑running with the same config gives consistent scores.

---

## 10. Acceptance criteria

CR‑011 is complete when:

1. **Submission format** is defined and documented, with JSON schemas for `manifest.json` and summary metric files.  
2. A **benchmark hub service** exists (even on localhost) with:
   - `POST /submit` and `GET /leaderboard/{experiment_id}` implemented.  
   - Results stored in a DB.  
3. The **CR‑010 demo stack** can:
   - Run E1–E3 on synthetic data,  
   - Produce a valid submission bundle,  
   - Upload it and appear on the leaderboard.  
4. At least one **baseline method** and one **H3LIX variant** are registered, showing different performance profiles.  
5. `BENCHMARK_HUB.md` + updated `EXPERIMENTS.md` clearly describe how this external benchmark ties back to the theoretical constructs: MPG, Rogue Variables, MUFS, coherence, and the experimental strategy in Section 6.  [oai_citation:7‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

If you want to keep pushing after CR‑011, the natural “final boss” is CR‑012: designing a **multi‑lab replication protocol** (OSF‑style) with shared configs, pre‑registered analysis plans, and a process for aggregating results across independent groups to test MPG‑Intuition and coherence claims under genuinely diverse implementations.
