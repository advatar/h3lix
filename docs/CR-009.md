**Change Request: CR‑009 – Formal Evaluation, Benchmarks & Ablation Suite for H3LIX / LAIZA**

---

## 1. Objective

Build a **formal evaluation and benchmarking layer** around everything you’ve implemented in CR‑001–CR‑008 so you can:

1. Quantitatively compare H3LIX/LAIZA against **simpler baselines** (e.g., plain LLM agents, RL agents, no‑Somatic/no‑Noetic ablations).  
2. Test the key claims in the paper about **MPG, RVs, MUFS, coherence, and policies** in a statistically solid, pre‑registered way.  [oai_citation:0‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
3. Make the whole experimental program **repeatable and shareable** (configs, logs, analysis notebooks), not just one‑off scripts.

This CR is about turning your architecture into something you can **publish and defend**: consistent experiments, ablations, and clear metrics.

---

## 2. Scope

### In scope

- A **benchmarking framework** with:
  - Experiment registry in Neo4j (or Postgres) with `:Experiment`, `:Run`, `:Condition`, `:MetricResult`.
  - YAML/JSON **experiment configs** for tasks, conditions, and models.
  - Python runner scripts that:
    - spin up H3LIX in different modes (full, ablated, baseline),  
    - collect MUFS/coherence/RV/policy data,  
    - log results to the registry.
- A suite of **core experiments**:
  - E1 – MPG‑Intuition & MUFS vs baselines.  [oai_citation:1‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
  - E2 – Coherence vs accuracy/calibration.  
  - E3 – Rogue Variables & Potency vs random segments.  [oai_citation:2‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  
  - E4 – Policy learning vs heuristic/static strategies.  
  - E5 – Generalization across tasks & domains.
- Notebooks and small dashboards to visualize results.

### Out of scope

- New tasks / domains beyond what you already have (we’ll assume a few decision tasks exist).  
- Multi‑lab infra (though CR‑009 prepares for that).

---

## 3. Theoretical anchors (what we’re testing)

From the paper, the key empirical claims we want to test are:  [oai_citation:3‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

1. **MPG & RVs**:  
   - Segments and pathways as structural units explain prediction–observation gaps better than raw features.  
   - RVs (via Shapley + 3σ) plus Potency are useful for **early warning, prioritization, targeted action, re‑coherence, and governance**.

2. **MPG‑Intuition & MUFS**:  
   - Under restricted awareness (IU/PU), trials where a nonempty MUFS exists are “MPG‑Intuitive” and should show **preserved or improved accuracy/calibration** at high coherence.  

3. **Coherence**:  
   - Noetic coherence (correlation/entropy/coherence spectra) marks states where bodily, symbolic, and outcome signals “line up”, and such states **predict better decision quality**, especially when deliberation is constrained.

4. **Policies & interventions**:  
   - Using RV Potency + coherence to guide interventions (CR‑007/008) can **improve or maintain performance and safety** vs no‑policy or naive heuristics.

CR‑009 gives you the machinery to actually check those statements.

---

## 4. Benchmark design

### 4.1 Core conditions

For each task family, you’ll compare:

- **C0: Baseline agent (no H3LIX)**  
  - Plain LLM/RL agent; no somatic, MPG, RV, Noetic layers.

- **C1: Symbolic‑only**  
  - LAIZA record & belief state, but **no Somatic, Noetic, or MPG / RV**.

- **C2: +Somatic**  
  - Somatic included → features into Symbolic, but **no MPG / Noetic**.

- **C3: +MPG + RVs**  
  - Full MPG structure & RV detection influencing analysis, but **Noetic coherence only logged**, not acted upon.

- **C4: Full H3LIX (SORK‑N)**  
  - Somatic + Symbolic + Noetic + MPG + RV + MUFS, Noetic feedback used to alter thresholds/attention (CR‑003–004).

- **C5: Full H3LIX + Policy engine**  
  - Add policy learning & meta‑policies (CR‑007–008).

All use the **same tasks / stimuli / timing**, as in the experimental strategy section.  [oai_citation:4‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

### 4.2 Outcome metrics

Per condition:

- Decision metrics:
  - Accuracy, AUC, log‑loss.
  - Calibration (Brier score, reliability diagrams).  
- Intuition metrics:
  - MUFS incidence (`P(has_mufs)`), MUFS size distribution.
  - MPG‑Intuition rate (`P(mpg_intuitive)`).
- Coherence metrics:
  - C(t) distribution, ΔC between blocks.
  - Correlations between C and accuracy, C and calibration error.
- Structural metrics:
  - RV detection rate, Potency distribution.
  - Effects of ablating top‑Potency vs random segments.
- Policy metrics:
  - Reward (as defined in CR‑007).
  - Override rate, safety flags.

---

## 5. Experiment registry & config system

### 5.1 Neo4j schema

New labels:

- `:Experiment`
- `:ExperimentCondition`
- `:ExperimentRun`
- `:MetricResult`

Constraints:

```cypher
CREATE CONSTRAINT experiment_id IF NOT EXISTS
FOR (e:Experiment) REQUIRE e.id IS UNIQUE;

CREATE CONSTRAINT expt_condition_id IF NOT EXISTS
FOR (c:ExperimentCondition) REQUIRE c.id IS UNIQUE;

CREATE CONSTRAINT expt_run_id IF NOT EXISTS
FOR (r:ExperimentRun) REQUIRE r.id IS UNIQUE;

CREATE CONSTRAINT metricresult_id IF NOT EXISTS
FOR (m:MetricResult) REQUIRE m.id IS UNIQUE;
```

Examples:

```cypher
CREATE (e:Experiment {
  id: $eid,
  name: "E1_MPG_Intuition_vs_Baselines",
  description: "MPG-Intuition tests under full vs restricted awareness",
  prereg_link: $osf_url,   // optional
  created_at: datetime()
});

CREATE (c:ExperimentCondition {
  id: $cid,
  name: "C4_Full_H3LIX",
  stack: "H3LIX_FULL",
  awareness_mode: "RESTRICTED",
  notes: "IU+PU"
})
CREATE (e)-[:HAS_CONDITION]->(c);
```

Each **run**:

```cypher
MATCH (c:ExperimentCondition {id: $cid})
CREATE (r:ExperimentRun {
  id: $rid,
  seed: $seed,
  started_at: datetime(),
  status: "RUNNING",
  n_trials: $n_trials
})
CREATE (c)-[:HAS_RUN]->(r);
```

Metric results:

```cypher
MATCH (r:ExperimentRun {id: $rid})
CREATE (m:MetricResult {
  id: $mid,
  name: "accuracy",
  value: $value,
  ci_lower: $ci_lower,
  ci_upper: $ci_upper,
  p_value: $p_value,
  details: $json_blob,
  created_at: datetime()
})
CREATE (r)-[:HAS_METRIC]->(m);
```

### 5.2 Config files

Define experiments in YAML, e.g. `configs/e1_mpg_intuition.yaml`:

```yaml
id: E1_MPG_Intuition
description: MPG-Intuition & MUFS vs baselines
tasks:
  - name: binary_intuition_task
    dataset: "datasets/binary_task_v1.jsonl"
conditions:
  - id: C0
    stack: BASELINE
    awareness_mode: FULL
  - id: C3
    stack: H3LIX_MPG_RV
    awareness_mode: RESTRICTED
  - id: C4
    stack: H3LIX_FULL
    awareness_mode: RESTRICTED
n_runs: 10
n_trials_per_run: 500
metrics:
  - accuracy
  - brier
  - has_mufs_rate
  - coherence_vs_accuracy_corr
```

Python runner parses these and writes everything to Neo4j.

---

## 6. Experiment families

### E1 – MPG‑Intuition & MUFS vs baselines

Purpose: empirically test **Definitions 7 & 8** (MUFS and MPG‑Intuition) against ablations.  [oai_citation:5‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

Design:

- Compare C0, C1, C3, C4 on restricted blocks (IU/PU).  
- For each trial:
  - Compute system decision (full vs restricted where applicable).
  - Perform MUFS search (CR‑004).
  - Label trial `mpg_intuitive`.

Key readouts:

- MUFS incidence `P(has_mufs | condition)`.  
- Flip probability between full and restricted decisions.  
- Accuracy & calibration **conditional on** `mpg_intuitive`.

Hypothesis:

- C4 (full H3LIX) shows:
  - higher MUFS incidence under restriction than full,  
  - better accuracy & calibration on `mpg_intuitive` trials than on non‑intuitive restricted trials, matching Section 6 predictions.  [oai_citation:6‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

---

### E2 – Coherence vs performance

Purpose: test Noetic claim that high coherence episodes correlate with better decision quality.  [oai_citation:7‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

Design:

- Run C2, C3, C4, C5.  
- For each trial:
  - Compute coherence C(t) (CR‑003 metrics).
  - Bin trials into low/medium/high coherence.

Metrics:

- Accuracy, calibration, reaction time by coherence bin.  
- Regression / mixed models: `performance ~ coherence + condition + task_difficulty`.

Hypothesis:

- Positive relationship between coherence and:
  - accuracy,
  - calibration quality,
  - and beneficial MUFS (intuitive trials that *help* performance).

---

### E3 – Rogue Variables & Potency ablations

Purpose: prove that the **Potency‑ranked RVs** matter more than random segments.  [oai_citation:8‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

Design:

- For C3 or C4, run conditions:

  - **A0**: no ablation.  
  - **A1**: ablate top‑K highest Potency segments (PU).  
  - **A2**: ablate K random segments with same size distribution (control).

Metrics:

- Δaccuracy, Δcalibration vs A0.  
- Change in MUFS incidence / size.  
- Change in coherence patterns.

Hypothesis:

- A1 produces larger performance drop & coherence disruption than A2 → Potency is meaningful.

---

### E4 – Policy learning vs heuristics

Purpose: validate CR‑007 policies & CR‑008 meta‑policies.

Design:

- Compare:

  - **P0**: no interventions.  
  - **P1**: static heuristic (e.g., always slow decisions when HRV low).  
  - **P2**: learned policy (bandit).  
  - **P3**: learned policy under meta‑policy control (trust‑aware).

Metrics:

- Policy reward, Δcoherence, Δaccuracy, Δcalibration.  
- Override rates, safety flags.

Hypothesis:

- P2/P3 ≥ P1 ≥ P0 on reward/coherence without worse safety; P3 has lower override & safety issues than P2.

---

### E5 – Cross‑task generalization

Purpose: test if MPG, RVs, and policies generalize across task families (classification, planning, social judgment, etc.).

Design:

- Train structure / policies on Task set A, test on Task set B.  
- Compare to baseline agent that doesn’t use MPG/RV.

Metrics:

- Generalization gap: `perf_B - perf_A`.  
- Structural reuse: how often segments / CollectiveSegments built on A help on B.

---

## 7. Pre‑registration & power analysis

To align with the paper’s emphasis on **rigor, pre‑registration, and reproducibility**, and referencing the preregistration literature cited there, you’ll:  [oai_citation:9‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

- Use a template (Markdown/YAML) with:

  - Hypotheses, conditions, primary/secondary metrics.  
  - Analysis plan (tests, corrections for multiple comparisons).  
  - Stopping rules, seeds, inclusion/exclusion criteria.

- Implement a **simulation / power analysis** script per experiment family:
  - Monte‑Carlo simulate effect sizes, estimate required N (runs, trials) to detect expected differences (e.g., MUFS rate differences, Δaccuracy) at chosen power (e.g., 0.8).

- Store prereg links (`Experiment.prereg_link`) and freeze configs before running main data collection.

---

## 8. Reporting & dashboards

Create:

- `analysis/e1_e2_intuition_coherence.ipynb` – for MPG‑Intuition & coherence tests.  
- `analysis/e3_rv_ablation.ipynb` – Potency vs random ablation results.  
- `analysis/e4_e5_policies_generalization.ipynb` – policy & cross‑task experiments.

And a small dashboard (e.g., Streamlit or simple front‑end) that:

- Queries Neo4j for `Experiment`→`Run`→`MetricResult`.  
- Shows:

  - forest plots for metrics per condition,  
  - MUFS incidence charts,  
  - coherence vs performance graphs,  
  - Potency ablation summaries.

---

## 9. How to run CR‑009

1. Add Neo4j schema for experiment entities.  
2. Implement:

   - `experiments/registry.py` – helpers for creating Experiments/Runs/MetricResults.  
   - `experiments/runner.py` – loads YAML config, orchestrates trials via Mirror Core variants and logging.  

3. Define the five experiment families (E1–E5) in `configs/`.  
4. Write analysis notebooks and basic dashboards.  
5. Run small pilot experiments to validate the pipeline, then main pre‑registered runs.

---

## 10. Acceptance criteria

CR‑009 is complete when:

1. **Experiment registry is live**  
   - Neo4j holds `:Experiment`, `:ExperimentCondition`, `:ExperimentRun`, `:MetricResult` nodes linked as described.  
   - YAML configs can be run end‑to‑end to populate them.

2. **At least three experiment families** (E1–E3) have full pipelines:
   - config → runs → metrics → analysis notebook plots.

3. **MPG‑Intuition, coherence, and RV claims are testable**:
   - You can run scripts to get:
     - MUFS incidence & MPG‑Intuition vs conditions,  
     - coherence vs performance curves,  
     - Potency vs random ablation outcomes.

4. **Pre‑registration & power analysis**:
   - Templates exist and are filled in for at least one experiment family.  
   - Power analysis scripts run and inform N.

5. **Reproducibility**:
   - Re‑running the same config with same seeds reproduces metrics within expected noise.  
   - Changing condition (e.g., removing Noetic layer) changes metrics in interpretable ways.

6. **Documentation**:
   - CR‑009 is saved as `docs/CR-009-Benchmarking-and-Ablations.md` and references the theoretical sections (MPG, RV, MUFS, coherence, experimental strategy) from the paper.  [oai_citation:10‡Symbiotic_human_AI_architecture.pdf](sediment://file_000000000db071f49f637b00e8081106)  

If you want to keep going even beyond this, the “next layer” would be CR‑010: turning all of this into a **reproducible open‑science package** (Docker images, public configs, and an anonymized demo dataset) so other labs can run the same MPG‑Intuition benchmarks and compare results.
